{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add custom concatenation of txt files controlled by some config to avoid saturation of topics\n",
    "# this is temporary\n",
    "\n",
    "TEXT_FILE_PATH = '/home/penguin/GeorgianWritingWizard/data/geo_corpus.txt'\n",
    "OUT_TEXT_FILE_PATH = '/home/penguin/GeorgianWritingWizard/data/stripped_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_FILE_PATH, 'r', encoding='utf-8') as f, \\\n",
    "     open(OUT_TEXT_FILE_PATH, 'w', encoding='utf-8') as out:\n",
    "    for line in f:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        out.write(unicodedata.normalize(\"NFKD\", line.strip()) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = '/home/penguin/GeorgianWritingWizard/data/stripped_data.txt'\n",
    "cc = '/home/penguin/GeorgianWritingWizard/data/out.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cc, 'w', encoding='utf-8') as out_file:\n",
    "    with open(pp, 'rb') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = line.decode('utf-8')\n",
    "                out_file.write(data.strip() + '\\n')\n",
    "            except Exception as e:\n",
    "                raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5b57fb68f4ab2fbe\n",
      "Found cached dataset text (/home/penguin/.cache/huggingface/datasets/text/default-5b57fb68f4ab2fbe/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fcfeba132d459a93c91cb6b3c4676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset('text', data_files=['/home/penguin/GeorgianWritingWizard/data/whole_corpus/georgian_large_corpus_cl2.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dedublicate import deduplicate_huggingface_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 41886434\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING CHUNK FROM: 0 TO 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:31<00:00, 64350.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpu_u9sbto/out.table.bin --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.0-12414726 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.12314726-24729452 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.24629452-37044178 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.36944178-49358904 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.49258904-61673630 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.61573630-73988356 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.73888356-86303082 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.86203082-98617808 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.98517808-110932534 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.110832534-123247260 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.123147260-135561986 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.135461986-147876712 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.147776712-160191438 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.160091438-172506164 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.172406164-184820890 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.184720890-197135616 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.197035616-209450342 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.209350342-221765068 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.221665068-234079794 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.233979794-246394520 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.246294520-258709246 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.258609246-271023972 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.270923972-283338698 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.283238698-295653424 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.295553424-307968150 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.307868150-320282876 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.320182876-332597602 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.332497602-344912328 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.344812328-357227054 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.357127054-369541780 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.369441780-381856506 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.381756506-394171232 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.394071232-406485958 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.406385958-418800684 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.418700684-431115410 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.431015410-443430136 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.443330136-455744862 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.455644862-468059588 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.467959588-480374314 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.480274314-492689040 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.492589040-505003766 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.504903766-517318492 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.517218492-529633218 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.529533218-541947944 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.541847944-554262670 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.554162670-566577396 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.566477396-578892122 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.578792122-591206848 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.591106848-603521574 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.603421574-615836300 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.615736300-628151026 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.628051026-640465752 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.640365752-652780478 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.652680478-665095204 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.664995204-677409930 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.677309930-689724656 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.689624656-702039382 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.701939382-714354108 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.714254108-726668834 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.726568834-738983560 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.738883560-751298286 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.751198286-763613012 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.763513012-775927738 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.775827738-788242464 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.788142464-800557190 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.800457190-812871916 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.812771916-825186642 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.825086642-837501368 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.837401368-849816094 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.849716094-862130820 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.862030820-874445546 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.874345546-886760272 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.886660272-899074998 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.898974998-911389724 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.911289724-923704450 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.923604450-936019176 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.935919176-948333902 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.948233902-960648628 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.960548628-972963354 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.972863354-985278080 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.985178080-997592806 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.997492806-1009907532 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1009807532-1022222258 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1022122258-1034536984 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1034436984-1046851710 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1046751710-1059166436 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1059066436-1071481162 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1071381162-1083795888 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1083695888-1096110614 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1096010614-1108425340 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1108325340-1120740066 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1120640066-1133054792 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1132954792-1145369518 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1145269518-1157684244 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1157584244-1169998970 --suffix-path /tmp/tmpu_u9sbto/tmp_full_dataset_as_text.part.1169898970-1182213710 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 1254043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 1254043/1254043 [00:07<00:00, 167473.89it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cd085a2ad34d03b54fe10eda9c1a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1367546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 2000000 TO 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:37<00:00, 52721.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpq32pg7n9/out.table.bin --suffix-path /tmp/tmpq32pg7n9/tmp_full_dataset_as_text.part.0-238013721 --suffix-path /tmp/tmpq32pg7n9/tmp_full_dataset_as_text.part.237913721-475927442 --suffix-path /tmp/tmpq32pg7n9/tmp_full_dataset_as_text.part.475827442-713841163 --suffix-path /tmp/tmpq32pg7n9/tmp_full_dataset_as_text.part.713741163-951654886 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 1012231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 1012231/1012231 [00:06<00:00, 154822.67it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432632b4907d4f12a92356cb99a42d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1404284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 4000000 TO 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:33<00:00, 59007.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpxy271uhx/out.table.bin --suffix-path /tmp/tmpxy271uhx/tmp_full_dataset_as_text.part.0-138536081 --suffix-path /tmp/tmpxy271uhx/tmp_full_dataset_as_text.part.138436081-276972162 --suffix-path /tmp/tmpxy271uhx/tmp_full_dataset_as_text.part.276872162-415408243 --suffix-path /tmp/tmpxy271uhx/tmp_full_dataset_as_text.part.415308243-553744325 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 127515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 127515/127515 [00:01<00:00, 125278.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f499213644a74d5fa9a3870ed96f4d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1599003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 6000000 TO 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:30<00:00, 66359.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmptanfv0q8/out.table.bin --suffix-path /tmp/tmptanfv0q8/tmp_full_dataset_as_text.part.0-239543882 --suffix-path /tmp/tmptanfv0q8/tmp_full_dataset_as_text.part.239443882-478987764 --suffix-path /tmp/tmptanfv0q8/tmp_full_dataset_as_text.part.478887764-718431646 --suffix-path /tmp/tmptanfv0q8/tmp_full_dataset_as_text.part.718331646-957775531 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 222112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 222112/222112 [00:01<00:00, 123764.14it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc8c55f23164ad29e632472679eaff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1642914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 8000000 TO 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:32<00:00, 62215.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpathjkbbx/out.table.bin --suffix-path /tmp/tmpathjkbbx/tmp_full_dataset_as_text.part.0-237297132 --suffix-path /tmp/tmpathjkbbx/tmp_full_dataset_as_text.part.237197132-474494264 --suffix-path /tmp/tmpathjkbbx/tmp_full_dataset_as_text.part.474394264-711691396 --suffix-path /tmp/tmpathjkbbx/tmp_full_dataset_as_text.part.711591396-948788530 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 194108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 194108/194108 [00:04<00:00, 39217.67it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c6f78ac48b40e183957e8b8400d8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1640887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 10000000 TO 12000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:30<00:00, 65719.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpmh2jvlqn/out.table.bin --suffix-path /tmp/tmpmh2jvlqn/tmp_full_dataset_as_text.part.0-164730543 --suffix-path /tmp/tmpmh2jvlqn/tmp_full_dataset_as_text.part.164630543-329361086 --suffix-path /tmp/tmpmh2jvlqn/tmp_full_dataset_as_text.part.329261086-493991629 --suffix-path /tmp/tmpmh2jvlqn/tmp_full_dataset_as_text.part.493891629-658522172 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 76412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 76412/76412 [00:01<00:00, 72885.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d83aa8c0b54ccfb614d5d906fcc1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1603978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 12000000 TO 14000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:30<00:00, 66256.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpwokbta6k/out.table.bin --suffix-path /tmp/tmpwokbta6k/tmp_full_dataset_as_text.part.0-165741056 --suffix-path /tmp/tmpwokbta6k/tmp_full_dataset_as_text.part.165641056-331382112 --suffix-path /tmp/tmpwokbta6k/tmp_full_dataset_as_text.part.331282112-497023168 --suffix-path /tmp/tmpwokbta6k/tmp_full_dataset_as_text.part.496923168-662564224 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 87505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 87505/87505 [00:01<00:00, 79313.74it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a4b9ded86347a5bc0a39849382f74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1589846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 14000000 TO 16000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:32<00:00, 62284.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmp9dskjcfr/out.table.bin --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.0-10786057 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.10686057-21472114 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.21372114-32158171 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.32058171-42844228 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.42744228-53530285 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.53430285-64216342 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.64116342-74902399 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.74802399-85588456 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.85488456-96274513 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.96174513-106960570 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.106860570-117646627 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.117546627-128332684 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.128232684-139018741 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.138918741-149704798 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.149604798-160390855 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.160290855-171076912 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.170976912-181762969 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.181662969-192449026 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.192349026-203135083 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.203035083-213821140 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.213721140-224507197 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.224407197-235193254 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.235093254-245879311 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.245779311-256565368 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.256465368-267251425 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.267151425-277937482 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.277837482-288623539 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.288523539-299309596 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.299209596-309995653 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.309895653-320681710 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.320581710-331367767 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.331267767-342053824 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.341953824-352739881 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.352639881-363425938 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.363325938-374111995 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.374011995-384798052 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.384698052-395484109 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.395384109-406170166 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.406070166-416856223 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.416756223-427542280 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.427442280-438228337 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.438128337-448914394 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.448814394-459600451 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.459500451-470286508 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.470186508-480972565 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.480872565-491658622 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.491558622-502344679 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.502244679-513030736 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.512930736-523716793 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.523616793-534402850 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.534302850-545088907 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.544988907-555774964 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.555674964-566461021 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.566361021-577147078 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.577047078-587833135 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.587733135-598519192 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.598419192-609205249 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.609105249-619891306 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.619791306-630577363 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.630477363-641263420 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.641163420-651949477 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.651849477-662635534 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.662535534-673321591 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.673221591-684007648 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.683907648-694693705 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.694593705-705379762 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.705279762-716065819 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.715965819-726751876 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.726651876-737437933 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.737337933-748123990 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.748023990-758810047 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.758710047-769496104 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.769396104-780182161 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.780082161-790868218 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.790768218-801554275 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.801454275-812240332 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.812140332-822926389 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.822826389-833612446 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.833512446-844298503 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.844198503-854984560 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.854884560-865670617 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.865570617-876356674 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.876256674-887042731 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.886942731-897728788 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.897628788-908414845 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.908314845-919100902 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.919000902-929786959 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.929686959-940473016 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.940373016-951159073 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.951059073-961845130 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.961745130-972531187 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.972431187-983217244 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.983117244-993903301 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.993803301-1004589358 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.1004489358-1015275415 --suffix-path /tmp/tmp9dskjcfr/tmp_full_dataset_as_text.part.1015175415-1025861504 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 1132528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 1132528/1132528 [00:05<00:00, 212343.00it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65fe08a81fa473280299e97d052f976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1096142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 16000000 TO 18000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:29<00:00, 67892.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpokik3bn5/out.table.bin --suffix-path /tmp/tmpokik3bn5/tmp_full_dataset_as_text.part.0-150882335 --suffix-path /tmp/tmpokik3bn5/tmp_full_dataset_as_text.part.150782335-301664670 --suffix-path /tmp/tmpokik3bn5/tmp_full_dataset_as_text.part.301564670-452447005 --suffix-path /tmp/tmpokik3bn5/tmp_full_dataset_as_text.part.452347005-603129343 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 356960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 356960/356960 [00:00<00:00, 364213.61it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6857bc51131945d9aeadf695d5ace176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/729134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 18000000 TO 20000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:28<00:00, 69185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpglhf1bnc/out.table.bin --suffix-path /tmp/tmpglhf1bnc/tmp_full_dataset_as_text.part.0-126490668 --suffix-path /tmp/tmpglhf1bnc/tmp_full_dataset_as_text.part.126390668-252881336 --suffix-path /tmp/tmpglhf1bnc/tmp_full_dataset_as_text.part.252781336-379272004 --suffix-path /tmp/tmpglhf1bnc/tmp_full_dataset_as_text.part.379172004-505562675 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 211590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 211590/211590 [00:00<00:00, 335912.89it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05575516c8694692bedb89b153721425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/495139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 20000000 TO 22000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:28<00:00, 69647.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpnl85v3th/out.table.bin --suffix-path /tmp/tmpnl85v3th/tmp_full_dataset_as_text.part.0-127845262 --suffix-path /tmp/tmpnl85v3th/tmp_full_dataset_as_text.part.127745262-255590524 --suffix-path /tmp/tmpnl85v3th/tmp_full_dataset_as_text.part.255490524-383335786 --suffix-path /tmp/tmpnl85v3th/tmp_full_dataset_as_text.part.383235786-510981051 --num-threads 6\n"
     ]
    }
   ],
   "source": [
    "last_i = 0\n",
    "batch_size = 2_000_000\n",
    "# batch_size = 100_000\n",
    "counter = 0\n",
    "for i in range(0, 41886434, batch_size):\n",
    "    counter += 1\n",
    "    print(f\"PROCESSING CHUNK FROM: {i} TO {i + batch_size}\")\n",
    "    chunk = data['train'].select(range(i, i + batch_size))\n",
    "    new_dataset = deduplicate_huggingface_dataset(chunk, original_cwd='/home/penguin/GeorgianWritingWizard/data/cramming')\n",
    "    new_dataset.save_to_disk(f'dedub_dataset/p{counter}')\n",
    "    last_i = i\n",
    "    print(\"=========================DONE=================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING CHUNK FROM: 20000000 TO 22000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:26<00:00, 74389.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpgx5tkuwj/out.table.bin --suffix-path /tmp/tmpgx5tkuwj/tmp_full_dataset_as_text.part.0-127845262 --suffix-path /tmp/tmpgx5tkuwj/tmp_full_dataset_as_text.part.127745262-255590524 --suffix-path /tmp/tmpgx5tkuwj/tmp_full_dataset_as_text.part.255490524-383335786 --suffix-path /tmp/tmpgx5tkuwj/tmp_full_dataset_as_text.part.383235786-510981051 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 216696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 216696/216696 [00:00<00:00, 344424.57it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99da351181a64694847217de63db61e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/515770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 22000000 TO 24000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:33<00:00, 59021.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpk01nd4mf/out.table.bin --suffix-path /tmp/tmpk01nd4mf/tmp_full_dataset_as_text.part.0-127306702 --suffix-path /tmp/tmpk01nd4mf/tmp_full_dataset_as_text.part.127206702-254513404 --suffix-path /tmp/tmpk01nd4mf/tmp_full_dataset_as_text.part.254413404-381720106 --suffix-path /tmp/tmpk01nd4mf/tmp_full_dataset_as_text.part.381620106-508826809 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 214234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 214234/214234 [00:00<00:00, 321460.18it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc86b99a14634844ae3f89f9be024b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/508465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 24000000 TO 26000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:34<00:00, 58635.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpi903w1rc/out.table.bin --suffix-path /tmp/tmpi903w1rc/tmp_full_dataset_as_text.part.0-130085291 --suffix-path /tmp/tmpi903w1rc/tmp_full_dataset_as_text.part.129985291-260070582 --suffix-path /tmp/tmpi903w1rc/tmp_full_dataset_as_text.part.259970582-390055873 --suffix-path /tmp/tmpi903w1rc/tmp_full_dataset_as_text.part.389955873-519941164 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 207501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 207501/207501 [00:00<00:00, 219592.53it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772f4873ec3b45a5b68e3b4e7a7d14f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/507575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 26000000 TO 28000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:33<00:00, 60069.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpk2pdctgi/out.table.bin --suffix-path /tmp/tmpk2pdctgi/tmp_full_dataset_as_text.part.0-128932173 --suffix-path /tmp/tmpk2pdctgi/tmp_full_dataset_as_text.part.128832173-257764346 --suffix-path /tmp/tmpk2pdctgi/tmp_full_dataset_as_text.part.257664346-386596519 --suffix-path /tmp/tmpk2pdctgi/tmp_full_dataset_as_text.part.386496519-515328695 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 210370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 210370/210370 [00:00<00:00, 223332.52it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f58a0c9806b4841bccf62b1776f71e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/501051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 28000000 TO 30000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:51<00:00, 39207.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpe83wxzft/out.table.bin --suffix-path /tmp/tmpe83wxzft/tmp_full_dataset_as_text.part.0-126748684 --suffix-path /tmp/tmpe83wxzft/tmp_full_dataset_as_text.part.126648684-253397368 --suffix-path /tmp/tmpe83wxzft/tmp_full_dataset_as_text.part.253297368-380046052 --suffix-path /tmp/tmpe83wxzft/tmp_full_dataset_as_text.part.379946052-506594739 --num-threads 6\n",
      "Now merging individual tables\n",
      "Cleaning up\n",
      "Finding self-similar parts...\n",
      "Collect self-similar from all parts...\n",
      "Number of removal tuples is 216924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing deduplicated data back to hf dataset: 100%|██████████| 216924/216924 [00:01<00:00, 178495.27it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8fbe2d3cc745cea0755c6fc24c5148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/513030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================DONE=================================\n",
      "PROCESSING CHUNK FROM: 30000000 TO 32000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing dataset to tmp files.: 100%|██████████| 2000000/2000000 [00:37<00:00, 53330.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition into parts and create suffix arrays...\n",
      "Waiting for jobs to finish\n",
      "Checking all wrote correctly\n",
      "Rerunning 0 jobs because they failed.\n",
      "Merging suffix trees\n",
      "/home/penguin/GeorgianWritingWizard/data/cramming/dedup/release/dedup_dataset merge --output-file /tmp/tmpoqtvwwn7/out.table.bin --suffix-path /tmp/tmpoqtvwwn7/tmp_full_dataset_as_text.part.0-129207570 --suffix-path /tmp/tmpoqtvwwn7/tmp_full_dataset_as_text.part.129107570-258315140 --suffix-path /tmp/tmpoqtvwwn7/tmp_full_dataset_as_text.part.258215140-387422710 --suffix-path /tmp/tmpoqtvwwn7/tmp_full_dataset_as_text.part.387322710-516430280 --num-threads 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPROCESSING CHUNK FROM: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m TO \u001b[39m\u001b[39m{\u001b[39;00mi \u001b[39m+\u001b[39m batch_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m chunk \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size))\n\u001b[0;32m----> 9\u001b[0m new_dataset \u001b[39m=\u001b[39m deduplicate_huggingface_dataset(chunk, original_cwd\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/penguin/GeorgianWritingWizard/data/cramming\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m new_dataset\u001b[39m.\u001b[39msave_to_disk(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdedub_dataset/p\u001b[39m\u001b[39m{\u001b[39;00mcounter\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m last_i \u001b[39m=\u001b[39m i\n",
      "File \u001b[0;32m~/GeorgianWritingWizard/geo-parser/geo_parser/preprocess/dedublicate.py:45\u001b[0m, in \u001b[0;36mdeduplicate_huggingface_dataset\u001b[0;34m(dataset, threshold, original_cwd)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mwith\u001b[39;00m tempfile\u001b[39m.\u001b[39mTemporaryDirectory() \u001b[39mas\u001b[39;00m tmpdir:\n\u001b[1;32m     44\u001b[0m     text_file \u001b[39m=\u001b[39m _write_tmp_file(dataset, dirname\u001b[39m=\u001b[39mtmpdir)\n\u001b[0;32m---> 45\u001b[0m     _make_suffix_array(text_file, tmpdir, path_to_rust_code)\n\u001b[1;32m     47\u001b[0m     \u001b[39m# Run other rust code directly\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     options \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m--length-threshold \u001b[39m\u001b[39m{\u001b[39;00mthreshold\u001b[39m}\u001b[39;00m\u001b[39m --cache-dir \u001b[39m\u001b[39m{\u001b[39;00mtmpdir\u001b[39m}\u001b[39;00m\u001b[39m/cache/\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/GeorgianWritingWizard/geo-parser/geo_parser/preprocess/dedublicate.py:131\u001b[0m, in \u001b[0;36m_make_suffix_array\u001b[0;34m(text_file, tmpdir, path_to_rust_code)\u001b[0m\n\u001b[1;32m    129\u001b[0m options \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m--output-file \u001b[39m\u001b[39m{\u001b[39;00mtmpdir\u001b[39m}\u001b[39;00m\u001b[39m/out.table.bin --suffix-path \u001b[39m\u001b[39m{\u001b[39;00mtorun\u001b[39m}\u001b[39;00m\u001b[39m --num-threads \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mget_num_threads()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_to_rust_code\u001b[39m}\u001b[39;00m\u001b[39m/dedup_dataset merge \u001b[39m\u001b[39m{\u001b[39;00moptions\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m os\u001b[39m.\u001b[39;49mpopen(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath_to_rust_code\u001b[39m}\u001b[39;49;00m\u001b[39m/dedup_dataset merge \u001b[39;49m\u001b[39m{\u001b[39;49;00moptions\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m    132\u001b[0m \u001b[39m# exit(0)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNow merging individual tables\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "last_i = 0\n",
    "batch_size = 2_000_000\n",
    "# batch_size = 100_000\n",
    "counter = 10\n",
    "for i in range(20_000_000, 41_886_434, batch_size):\n",
    "    counter += 1\n",
    "    print(f\"PROCESSING CHUNK FROM: {i} TO {i + batch_size}\")\n",
    "    chunk = data['train'].select(range(i, i + batch_size))\n",
    "    new_dataset = deduplicate_huggingface_dataset(chunk, original_cwd='/home/penguin/GeorgianWritingWizard/data/cramming')\n",
    "    new_dataset.save_to_disk(f'dedub_dataset/p{counter}')\n",
    "    last_i = i\n",
    "    print(\"=========================DONE=================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_from_disk('/home/penguin/GeorgianWritingWizard/geo-parser/geo_parser/preprocess/dedub_dataset/p10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 495139\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter += 1\n",
    "print(f\"PROCESSING CHUNK FROM: {last_i + batch_size} TO {41886434}\")\n",
    "chunk = data['train'].select(range(last_i + batch_size, 41886434))\n",
    "new_dataset = deduplicate_huggingface_dataset(chunk, original_cwd='/home/penguin/GeorgianWritingWizard/data/cramming')\n",
    "new_dataset.save_to_disk(f'dedub_dataset/p{counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sml_data = data['train'].select(range(3_000_000, 3_000_000 + batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = deduplicate_huggingface_dataset(sml_data, original_cwd='/home/penguin/GeorgianWritingWizard/data/cramming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk('dedub_dataset/p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ea76b4bf3641c0acc3a149168fe43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidConfigName",
     "evalue": "Bad characters from black list '<>:/\\|?*' found in '/home/penguin/GeorgianWritingWizard/data/whole_corpus/filter_v2/filtered.txt'. They could create issues when creating a directory for this config on Windows filesystem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidConfigName\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m/home/penguin/GeorgianWritingWizard/data/whole_corpus/filter_v2/filtered.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/load.py:1734\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1731\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[1;32m   1733\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1734\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1735\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1736\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1737\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1738\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1739\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1740\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1741\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1742\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1743\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1744\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1745\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1746\u001b[0m )\n\u001b[1;32m   1748\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/load.py:1518\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m   1517\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[1;32m   1519\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1520\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m   1521\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1522\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1523\u001b[0m     \u001b[39mhash\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mhash\u001b[39;49m,\n\u001b[1;32m   1524\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1525\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1526\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs,\n\u001b[1;32m   1527\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1528\u001b[0m )\n\u001b[1;32m   1530\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/builder.py:322\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m data_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data_dir\n\u001b[0;32m--> 322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_builder_config(\n\u001b[1;32m    323\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m    324\u001b[0m     custom_features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    325\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/builder.py:475\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m config_kwargs \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVERSION\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVERSION:\n\u001b[1;32m    474\u001b[0m         config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVERSION\n\u001b[0;32m--> 475\u001b[0m     builder_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBUILDER_CONFIG_CLASS(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs)\n\u001b[1;32m    477\u001b[0m \u001b[39m# otherwise use the config_kwargs to overwrite the attributes\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     builder_config \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(builder_config)\n",
      "File \u001b[0;32m<string>:14\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, name, version, data_dir, data_files, description, features, encoding, errors, chunksize, keep_linebreaks, sample_by)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/builder.py:125\u001b[0m, in \u001b[0;36mBuilderConfig.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m invalid_char \u001b[39min\u001b[39;00m INVALID_WINDOWS_CHARACTERS_IN_PATH:\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m invalid_char \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname:\n\u001b[0;32m--> 125\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidConfigName(\n\u001b[1;32m    126\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBad characters from black list \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mINVALID_WINDOWS_CHARACTERS_IN_PATH\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m found in \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThey could create issues when creating a directory for this config on Windows filesystem.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         )\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files, DataFilesDict):\n\u001b[1;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a DataFilesDict in data_files but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidConfigName\u001b[0m: Bad characters from black list '<>:/\\|?*' found in '/home/penguin/GeorgianWritingWizard/data/whole_corpus/filter_v2/filtered.txt'. They could create issues when creating a directory for this config on Windows filesystem."
     ]
    }
   ],
   "source": [
    "new_dataset = load_dataset('text', '/home/penguin/GeorgianWritingWizard/data/whole_corpus/filter_v2/filtered.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trash_off import filtering_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_setup = dict(\n",
    "    batched=True,\n",
    "    batch_size=1024,\n",
    "    num_proc=None,  # a bit messy but c4 in RAM can be overbearing otherwise\n",
    "    # load_from_cache_file=False,\n",
    "    # keep_in_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = new_dataset.filter(\n",
    "    filtering_rule,\n",
    "    desc=\"Filter sentences that cannot be tokessnized well.\",\n",
    "    **map_setup,\n",
    "    # keep_in_memory=True,  # can run out of mem even on the 750GB node?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.push_to_hub('ZurabDz/geo_small_corpus_dedublicated_trash_off', token='<>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.push_to_hub('ZurabDz/geo_small_corpus', token='<huggingface token>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('ZurabDz/geo_small_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/penguin/GeorgianWritingWizard/language_engine/examples/outputs/2023-03-09/22-36-38/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub('ZurabDz/GeoSentencePieceBPE_32768_v2', use_auth_token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AlbertConfig.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk('/home/penguin/GeorgianWritingWizard/language_engine/examples/processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.push_to_hub('ZurabDz/geo_small_corpus_tokenized_v2', token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/penguin/GeorgianWritingWizard/language_engine/examples/out/checkpoint-2400'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_sent(text, top_k=5):\n",
    "    # Tokenize input\n",
    "    text = \"<cls> %s <sep>\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    print(tokenized_text)\n",
    "    masked_index = tokenized_text.index(\"<mask>\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        print(\"<mask>: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_masked_sent(\"გამარჯობა რორგო ხარ <mask>\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"გამარჯობა რორგო ხარ <mask>\"\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_index = input_ids.index(tokenizer.mask_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.tensor([input_ids]))\n",
    "predictions = outputs[0][0, mask_token_index].topk(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in predictions.indices:\n",
    "    predicted_token = tokenizer.decode([token])\n",
    "    print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ZurabDz/GeoSentencePieceBPE_32768_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('ZurabDz/geo_small_corpus_dedublicated_trash_off_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ds['train'][10]['input_ids']\n",
    "tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.sub('[^ა-ჰ\\s-?.]|', '', '„აშშ-მა უნდა შეწყვიტოს მიუღებელი ქცევა და ამის მაგივრად რეალური ნაბიჯები გადადგას მისი ეკონომიკის აღსადგენად, რომელიც კორონავირუსის პანდემიის მარწუხებშია მოქცეული“, – აცხადებენ ჩინეთის ვაჭრობის სამინისტროში.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/penguin/GeorgianWritingWizard/data/stripped_data.txt', 'r', encoding='utf-8') as f, \\\n",
    "     open('/home/penguin/GeorgianWritingWizard/data/test_data.txt', 'w', encoding='utf-8') as out:\n",
    "    for line in f:\n",
    "        result = re.sub('[^ა-ჰ\\s\\n]|', '', line)\n",
    "        if len(result.strip()) == 0:\n",
    "            continue\n",
    "        if len(line.strip().split(' ')) < 10:\n",
    "            continue\n",
    "        out.write(result.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin/miniconda3/envs/research/lib/python3.8/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds = load_from_disk('/home/penguin/GeorgianWritingWizard/language_engine/examples/processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/penguin/GeorgianWritingWizard/language_engine/examples/outputs/2023-03-16/19-34-59/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'შაქრიანი დიაბეტი გართულებული სხვადასხვა ფორმით, კოვიდ ინფექციის შერწყმის შემთხვევაში მაღალი ლეტალობის ინდექსს არ წარმოადგენს, ამის შესახებ პირველი საუნივერსიტეტო კლინიკის ექიმმა ლევან რატიანმა განაცხადა.[SEP] ლევან რატიანმა ამის მაგალითად მოიყვანა კორონავირუსით ინფიცირებული 82 წლის ქალი, რომელსაც თანმხლები დაავადება შაქრიანი დიაბეტი აქვს.[SEP] ქალბატონი, რომელიც გვყავს 82 წლის შაქრიანი დიაბეტით, ორივე ქვემო კიდურის ამპუტაციით და მიკრო ანგიოპათიის გამო გართულებული პრობლემებით, მოიხსნა აპარატიდან, არის ადეკვატური, კონტაქტური, გემოდინამიკა, სუნთქვის პარამეტრები არის სტაბილური, რჩება ოქსიგენ დამოკიდებული. ეს მაგალითები რატომ მოვიყვანე ამ ქალბატონს ბოლო 3 წლის მანძილზე აქვს შაქრიანი დიაბეტი, იმყოფება ინსულინ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ZurabDz/GeoSentencePieceBPE_32768/commit/28c3a44b55c3e9650aaf0fe6539de1a2fa767e54', commit_message='Upload tokenizer', commit_description='', oid='28c3a44b55c3e9650aaf0fe6539de1a2fa767e54', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('ZurabDz/GeoSentencePieceBPE_32768', use_auth_token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31bef8f0e01d86822998708cae530d3f279653869977ee7676ea73d628f47b0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
