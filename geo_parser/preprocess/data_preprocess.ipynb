{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add custom concatenation of txt files controlled by some config to avoid saturation of topics\n",
    "# this is temporary\n",
    "\n",
    "TEXT_FILE_PATH = '/home/penguin/GeorgianWritingWizard/data/geo_corpus.txt'\n",
    "OUT_TEXT_FILE_PATH = '/home/penguin/GeorgianWritingWizard/data/stripped_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_FILE_PATH, 'r', encoding='utf-8') as f, \\\n",
    "     open(OUT_TEXT_FILE_PATH, 'w', encoding='utf-8') as out:\n",
    "    for line in f:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        out.write(unicodedata.normalize(\"NFKD\", line.strip()) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = '/home/penguin/GeorgianWritingWizard/data/stripped_data.txt'\n",
    "cc = '/home/penguin/GeorgianWritingWizard/data/out.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cc, 'w', encoding='utf-8') as out_file:\n",
    "    with open(pp, 'rb') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = line.decode('utf-8')\n",
    "                out_file.write(data.strip() + '\\n')\n",
    "            except Exception as e:\n",
    "                raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('text', data_files=['/home/penguin/GeorgianWritingWizard/data/stripped_data.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dedublicate import deduplicate_huggingface_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small = data['train'].select(range(0, 1500000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = deduplicate_huggingface_dataset(data['train'], original_cwd='/home/penguin/GeorgianWritingWizard/geo-parser/geo_parser/preprocess/cramming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.push_to_hub('ZurabDz/geo_small_corpus_dedublicated', token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = load_dataset('ZurabDz/geo_small_corpus_dedublicated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trash_off import filtering_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_setup = dict(\n",
    "    batched=True,\n",
    "    batch_size=1024,\n",
    "    num_proc=None,  # a bit messy but c4 in RAM can be overbearing otherwise\n",
    "    # load_from_cache_file=False,\n",
    "    # keep_in_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = new_dataset.filter(\n",
    "    filtering_rule,\n",
    "    desc=\"Filter sentences that cannot be tokessnized well.\",\n",
    "    **map_setup,\n",
    "    # keep_in_memory=True,  # can run out of mem even on the 750GB node?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.push_to_hub('ZurabDz/geo_small_corpus_dedublicated_trash_off', token='<>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.push_to_hub('ZurabDz/geo_small_corpus', token='<huggingface token>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('ZurabDz/geo_small_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/penguin/GeorgianWritingWizard/language_engine/examples/outputs/2023-03-09/22-36-38/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/home/penguin/GeorgianWritingWizard/language_engine/examples/outputs/2023-03-09/22-36-38/tokenizer', vocab_size=32768, model_max_length=128, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '[SEP]', 'pad_token': '<pad>', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ZurabDz/GeoSentencePieceBPE_32768_v2/commit/4dac219fa84781e2f61599c7a19e7f44188e59f1', commit_message='Upload tokenizer', commit_description='', oid='4dac219fa84781e2f61599c7a19e7f44188e59f1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('ZurabDz/GeoSentencePieceBPE_32768_v2', use_auth_token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AlbertConfig.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertConfig {\n",
       "  \"architectures\": [\n",
       "    \"AlbertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_dropout_prob\": 0.1,\n",
       "  \"down_scale_factor\": 1,\n",
       "  \"embedding_size\": 128,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"gap_size\": 0,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_dropout_prob\": 0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"inner_group_num\": 1,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"albert\",\n",
       "  \"net_structure_type\": 0,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_groups\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_memory_blocks\": 0,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.27.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin/miniconda3/envs/research/lib/python3.8/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds = load_from_disk('/home/penguin/GeorgianWritingWizard/language_engine/examples/processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d779018fae7e49098541459dfe133931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.push_to_hub('ZurabDz/geo_small_corpus_tokenized_v2', token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/penguin/GeorgianWritingWizard/language_engine/examples/out/checkpoint-2400'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/penguin/GeorgianWritingWizard/language_engine/examples/out/checkpoint-2400 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertModel were not initialized from the model checkpoint at /home/penguin/GeorgianWritingWizard/language_engine/examples/out/checkpoint-2400 and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertModel(\n",
       "  (embeddings): AlbertEmbeddings(\n",
       "    (word_embeddings): Embedding(32768, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (encoder): AlbertTransformer(\n",
       "    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (albert_layer_groups): ModuleList(\n",
       "      (0): AlbertLayerGroup(\n",
       "        (albert_layers): ModuleList(\n",
       "          (0): AlbertLayer(\n",
       "            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (attention): AlbertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attention_dropout): Dropout(p=0, inplace=False)\n",
       "              (output_dropout): Dropout(p=0, inplace=False)\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pooler_activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_sent(text, top_k=5):\n",
    "    # Tokenize input\n",
    "    text = \"<cls> %s <sep>\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    print(tokenized_text)\n",
    "    masked_index = tokenized_text.index(\"<mask>\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        print(\"<mask>: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', '▁გამარჯ', 'ობა', '▁რო', 'რ', 'გო', '▁ხარ', '▁', '<mask>', '▁', '<sep>']\n",
      "<mask>: 'ობა'  | weights: 0.021191157400608063\n",
      "<mask>: 'ס'  | weights: 0.01022934727370739\n",
      "<mask>: 'Σ'  | weights: 0.009252398274838924\n",
      "<mask>: '建'  | weights: 0.009034795686602592\n",
      "<mask>: '©'  | weights: 0.008261569775640965\n"
     ]
    }
   ],
   "source": [
    "predict_masked_sent(\"გამარჯობა რორგო ხარ <mask>\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"გამარჯობა რორგო ხარ <mask>\"\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_index = input_ids.index(tokenizer.mask_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.tensor([input_ids]))\n",
    "predictions = outputs[0][0, mask_token_index].topk(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ობა\n",
      "ס\n",
      "Σ\n",
      "建\n",
      "©\n"
     ]
    }
   ],
   "source": [
    "for token in predictions.indices:\n",
    "    predicted_token = tokenizer.decode([token])\n",
    "    print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ZurabDz/GeoSentencePieceBPE_32768_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ZurabDz--geo_small_corpus_dedublicated_trash_off_tokenized-0f4116458a62a258\n",
      "Found cached dataset parquet (/home/penguin/.cache/huggingface/datasets/ZurabDz___parquet/ZurabDz--geo_small_corpus_dedublicated_trash_off_tokenized-0f4116458a62a258/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bea7d2f43345ac87740b837589101b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('ZurabDz/geo_small_corpus_dedublicated_trash_off_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ულ გადაადგილების სისტემას იყენებდნენ.<sep> „ეს აღმოჩენა სრულიად ახალ გზას გვიხსნის დინოზავრების კვლევაში; ბევრ ცნობას მატებს როგორც უკვე არსებულ, ისე ახალ ნარატივს და დრამატულად ცვლის ჩვენს წარმოდგენებს დინოზავ“, — ამბობსაუკუნის წინ, მეცნიერები ვარაუდობდნენ, რომ ხმელეთის დინოზავრები შეიძლება წყლიან გარემოშიც ბინადრობდნენ, მაგრამ ბოლო ათწლეულებში ეს იდეა უკუაგდეს, რადგან კვლევათა უმეტესობა მიუთითებდა, რომ არამფრინავი დინოზავრები მხოლოდ ხმელეთზე სახლობდნენ.<sep> თუმცა, ამ საკითხს გარკვეულწილად ართულებდა სპინოზავრი, რადგან ზოგიერთი უძველესი ძვალი მიუთითებდა მტკიცებულებებზე, რომ ის შეიძლება, წყალთან სანახევროდ შეგუებული ყოფილიყო.<sep>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ds['train'][10]['input_ids']\n",
    "tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "bad character range \\s-? at position 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49msub(\u001b[39m'\u001b[39;49m\u001b[39m[^ა-ჰ\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms-?.]|\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m„აშშ-მა უნდა შეწყვიტოს მიუღებელი ქცევა და ამის მაგივრად რეალური ნაბიჯები გადადგას მისი ეკონომიკის აღსადგენად, რომელიც კორონავირუსის პანდემიის მარწუხებშია მოქცეული“, – აცხადებენ ჩინეთის ვაჭრობის სამინისტროში.\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/re.py:210\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sre_compile\u001b[39m.\u001b[39misstring(pattern):\n\u001b[1;32m    303\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfirst argument must be string or compiled pattern\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m p \u001b[39m=\u001b[39m sre_compile\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[1;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    307\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[1;32m    763\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> 764\u001b[0m     p \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[1;32m    765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/sre_parse.py:948\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    945\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m    947\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    949\u001b[0m \u001b[39mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    950\u001b[0m     \u001b[39m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    951\u001b[0m     \u001b[39m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[1;32m    952\u001b[0m     state \u001b[39m=\u001b[39m State()\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[1;32m    442\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    444\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    445\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    446\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/sre_parse.py:593\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m code1[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m LITERAL \u001b[39mor\u001b[39;00m code2[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m LITERAL:\n\u001b[1;32m    592\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbad character range \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (this, that)\n\u001b[0;32m--> 593\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(msg, \u001b[39mlen\u001b[39m(this) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(that))\n\u001b[1;32m    594\u001b[0m lo \u001b[39m=\u001b[39m code1[\u001b[39m1\u001b[39m]\n\u001b[1;32m    595\u001b[0m hi \u001b[39m=\u001b[39m code2[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31merror\u001b[0m: bad character range \\s-? at position 5"
     ]
    }
   ],
   "source": [
    "result = re.sub('[^ა-ჰ\\s-?.]|', '', '„აშშ-მა უნდა შეწყვიტოს მიუღებელი ქცევა და ამის მაგივრად რეალური ნაბიჯები გადადგას მისი ეკონომიკის აღსადგენად, რომელიც კორონავირუსის პანდემიის მარწუხებშია მოქცეული“, – აცხადებენ ჩინეთის ვაჭრობის სამინისტროში.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'აშშ-მა უნდა შეწყვიტოს მიუღებელი ქცევა და ამის მაგივრად რეალური ნაბიჯები გადადგას მისი ეკონომიკის აღსადგენად რომელიც კორონავირუსის პანდემიის მარწუხებშია მოქცეული  აცხადებენ ჩინეთის ვაჭრობის სამინისტროში'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/penguin/GeorgianWritingWizard/data/stripped_data.txt', 'r', encoding='utf-8') as f, \\\n",
    "     open('/home/penguin/GeorgianWritingWizard/data/test_data.txt', 'w', encoding='utf-8') as out:\n",
    "    for line in f:\n",
    "        result = re.sub('[^ა-ჰ\\s\\n]|', '', line)\n",
    "        if len(result.strip()) == 0:\n",
    "            continue\n",
    "        if len(line.strip().split(' ')) < 10:\n",
    "            continue\n",
    "        out.write(result.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31bef8f0e01d86822998708cae530d3f279653869977ee7676ea73d628f47b0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
